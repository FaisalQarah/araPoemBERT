{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb12e46f-23c8-4181-9e24-a0276dc12dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy==1.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4da4c2c-e801-4fa6-8676-c04e551ee88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.12\n",
    "!pip install pyarabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99b7f2d-e4a6-4dc1-9a9f-eb0d99dc95b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbd9b83-c345-489a-ba9d-c27dc601972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets\n",
    "!pip install datasets\n",
    "!pip install transformers[torch]\n",
    "!pip install nvidia-ml-py3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c51d229-d74c-4ce9-9910-786a65f81dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd46459-397c-4680-aa71-61f421963385",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import pyarabic.araby as araby\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccb60e6e-ebd4-4a2d-96f8-8b5f2c65d151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2090907"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2090907"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# cell-1  \n",
    "#load and clean the data (removing diacritics and unwanted text)\n",
    "\n",
    "df = pd.read_csv('poemsDataset.csv')\n",
    "df.fillna('', inplace=True)\n",
    "display(len(df))\n",
    "\n",
    "\n",
    "def remove_diacritics(a):    \n",
    "    return araby.strip_diacritics(a)\n",
    "\n",
    "df['first_hemistich'] = df['first_hemistich'].apply(remove_diacritics)\n",
    "df['second_hemistich'] = df['second_hemistich'].apply(remove_diacritics)\n",
    "\n",
    "def normalizeBeforeTraining(df):\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('النابغـة: ', '')\n",
    "    df['second_hemistich'] = df['second_hemistich'].str.replace('الـربيع: ', '')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('عبيــد: ', '')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('امـرؤ القيسـ: ', '')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('امرؤ القيس: ', '')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('(جلال الــــدين الــــرومي):', '')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('(لـوك الفيلسـوف الإنكليزي):', '')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('(كانت الفيلسوف الألماني ):', '')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('(بركســــــــــــــــون):', '')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('(الحـــــــــــــــور):', '')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('(الشــــــــــــــاعر):', '')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('(الإنســـــــــــــــان):', '')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('العلم):', '', regex=False)\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('(العشــــــــــــــــق):', '', regex=False)\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('(الزهــــــــــــــــــرة):', '', regex=False)\n",
    "    df['second_hemistich'] = df['second_hemistich'].str.replace('التوأم اليشكري: ', '', regex=False)  \n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('آ', 'أ')\n",
    "    df['second_hemistich'] = df['second_hemistich'].str.replace('آ', 'أ')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('[/\":?،؟]', '')\n",
    "    df['second_hemistich'] = df['second_hemistich'].str.replace('[/\":?،؟]', '')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('  ', ' ')\n",
    "    df['second_hemistich'] = df['second_hemistich'].str.replace('  ', ' ')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('  ', ' ')\n",
    "    df['second_hemistich'] = df['second_hemistich'].str.replace('  ', ' ')\n",
    "\n",
    "\n",
    "normalizeBeforeTraining(df)\n",
    "df.drop(df[(df['first_hemistich'] == '') & (df['second_hemistich'] == '')].index, inplace=True)\n",
    "\n",
    "#if first_hemistich == '', then copy the text from second_hemistich. then delete the text in the second_hemistich\n",
    "df['first_hemistich'] = df.apply(lambda x: x['second_hemistich'] if x['first_hemistich'] == '' else x['first_hemistich'], axis=1)\n",
    "df['second_hemistich'] = df.apply(lambda x: '' if x['first_hemistich'] == x['second_hemistich'] else x['second_hemistich'], axis=1)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "display(len(df))\n",
    "# display(df[:10])\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e68754b-15f2-4e08-b38a-ea0ccd974d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1850351"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1480280"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "370071"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cell-2 \n",
    "# preparing data for finetuning\n",
    "\n",
    "\n",
    "df['second_hemistich'].replace('', 'E', inplace=True)\n",
    "dfc = df[['first_hemistich', 'second_hemistich', 'meter', 'link']].copy()\n",
    "dfc['text'] = dfc['first_hemistich'] + ' S ' + dfc['second_hemistich']\n",
    "\n",
    "#removing verses without a meter\n",
    "dfc = dfc[dfc['meter'] != ''] \n",
    "dfc = dfc[dfc['meter'] != 'unspecified']\n",
    "dfc = dfc[dfc['meter'] != 'mixed']\n",
    "\n",
    "\n",
    "\n",
    "classic = ['taweel', 'kamel', 'baseet', 'khafif', 'wafer', 'rajaz', 'ramel', 'mutaqarib',\n",
    "           'saree', 'munsarih', 'mujtath', 'hazaj', 'madeed', 'mutadarak', 'muqtadab', 'mudari'] \n",
    "\n",
    "#including only verses with classical meters\n",
    "# dfc = dfc[dfc['meter'].isin(classic)]\n",
    "\n",
    "dfc.reset_index(drop=True, inplace=True)\n",
    "\n",
    "dfc['meter'] = dfc['meter'].astype('category')\n",
    "# display(dfc['meter'].unique())\n",
    "\n",
    "dfc['label'] = dfc['meter'].cat.codes #assign cat_value for each meter type\n",
    "dftrain, dftest = train_test_split(dfc, test_size=0.20, random_state=42, stratify=dfc['label'])\n",
    "ytrain = to_categorical(dftrain['label']).astype('int32')\n",
    "ytest = to_categorical(dftest['label']).astype('int32')\n",
    "\n",
    "max_sequence_length = 32\n",
    "train_batch_size = 256\n",
    "classes_num = len(dfc['meter'].unique())\n",
    "\n",
    "display(len(dfc))\n",
    "display(len(dftrain))\n",
    "display(len(dftest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdae2e7c-370e-407c-a87a-2e61f782f399",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell-3\n",
    "#loading the tokenizer and the model\n",
    "\n",
    "from transformers import AutoTokenizer,TFBertModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('aubmindlab/bert-base-arabert')\n",
    "bert = TFBertModel.from_pretrained('aubmindlab/bert-base-arabert', from_pt=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd261fd0-0b55-46b6-8f5a-67f213baeb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell-4\n",
    "#tokenizing the data\n",
    "\n",
    "xtrain = tokenizer(\n",
    "    text=dftrain['text'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length = max_sequence_length,\n",
    "    truncation=True,\n",
    "    padding='max_length', \n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids = False,\n",
    "    return_attention_mask = True,\n",
    "    verbose = True)\n",
    "\n",
    "\n",
    "xtest = tokenizer(\n",
    "    text=dftest['text'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length = max_sequence_length,\n",
    "    truncation=True,\n",
    "    padding='max_length', \n",
    "    return_tensors='tf',\n",
    "    return_token_type_ids = False,\n",
    "    return_attention_mask = True,\n",
    "    verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac8e3243-381f-4745-8b60-4f96974f7336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
       "array([    2,  2914,   277,   151,  2890,  1110,    83, 31553,  8365,\n",
       "           9, 25033,   122,    85,  4147,  5444,   678,   321, 40125,\n",
       "           3,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0], dtype=int32)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display(xtest)\n",
    "display(xtest['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cac648a8-7bf9-40b6-bc13-9f2aeaddf783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 32)]         0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 32)]         0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  135193344   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 32,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 16)           12304       ['tf_bert_model[0][1]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 135,205,648\n",
      "Trainable params: 135,205,648\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#cell-5\n",
    "# building classifier model\n",
    "\n",
    "input_ids = Input(shape=(max_sequence_length,), dtype=tf.int32, name=\"input_ids\")\n",
    "input_mask = Input(shape=(max_sequence_length,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "output = bert([input_ids, input_mask])[1] #pooled_output\n",
    "output = tf.keras.layers.Dense(classes_num, activation='softmax', name='output')(output)\n",
    "   \n",
    "model = tf.keras.Model(inputs=[input_ids, input_mask], outputs=output)\n",
    "\n",
    "optimizer = Adam(learning_rate=5e-05)\n",
    "\n",
    "\n",
    "loss =CategoricalCrossentropy(from_logits = True)\n",
    "metric = CategoricalAccuracy('balanced_accuracy'),\n",
    "\n",
    "model.compile(optimizer = optimizer, loss = loss, metrics = metric)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4c8469d-06ca-4f95-8dfc-8d72bc656a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_28651/403206910.py:1: experimental_run_functions_eagerly (from tensorflow.python.eager.polymorphic_function.quarantine) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.run_functions_eagerly` instead of the experimental version.\n"
     ]
    }
   ],
   "source": [
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a65491c-3faf-48ce-9e1a-750781892ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "5783/5783 [==============================] - 1870s 323ms/step - loss: 0.3733 - balanced_accuracy: 0.8765 - val_loss: 0.1075 - val_balanced_accuracy: 0.9702\n",
      "Epoch 2/6\n",
      "5783/5783 [==============================] - 1875s 324ms/step - loss: 0.0974 - balanced_accuracy: 0.9726 - val_loss: 0.0830 - val_balanced_accuracy: 0.9781\n",
      "Epoch 3/6\n",
      "5783/5783 [==============================] - 1882s 325ms/step - loss: 0.0721 - balanced_accuracy: 0.9808 - val_loss: 0.0737 - val_balanced_accuracy: 0.9813\n",
      "Epoch 4/6\n",
      "5783/5783 [==============================] - 1854s 321ms/step - loss: 0.0598 - balanced_accuracy: 0.9848 - val_loss: 0.0749 - val_balanced_accuracy: 0.9814\n",
      "Epoch 5/6\n",
      "5783/5783 [==============================] - 1835s 317ms/step - loss: 0.0515 - balanced_accuracy: 0.9873 - val_loss: 0.0729 - val_balanced_accuracy: 0.9824\n",
      "Epoch 6/6\n",
      "5783/5783 [==============================] - 1839s 318ms/step - loss: 0.0459 - balanced_accuracy: 0.9890 - val_loss: 0.0726 - val_balanced_accuracy: 0.9825\n"
     ]
    }
   ],
   "source": [
    "#train the model (classic meters)\n",
    "\n",
    "train_history = model.fit(\n",
    "    x ={'input_ids':xtrain['input_ids'],'attention_mask':xtrain['attention_mask']}, y = ytrain,\n",
    "    validation_data = ({'input_ids':xtest['input_ids'],'attention_mask':xtest['attention_mask']}, \n",
    "    ytest), epochs=6, batch_size=train_batch_size )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32c3f32b-4936-4324-b48b-a7f4eb0f03f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11565/11565 [==============================] - 609s 53ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9913    0.9933    0.9923     53009\n",
      "           1     0.8165    0.8838    0.8488      1833\n",
      "           2     0.9839    0.9867    0.9853     81382\n",
      "           3     0.9832    0.9920    0.9875     34821\n",
      "           4     0.9098    0.8912    0.9004      1755\n",
      "           5     0.5588    0.2639    0.3585        72\n",
      "           6     0.9575    0.9551    0.9563      3871\n",
      "           7     0.9753    0.9585    0.9668      5973\n",
      "           8     0.8920    0.8263    0.8579       190\n",
      "           9     0.9310    0.8650    0.8968      1326\n",
      "          10     0.9832    0.9788    0.9810     13905\n",
      "          11     0.9550    0.9262    0.9404     22267\n",
      "          12     0.9704    0.9768    0.9736     17971\n",
      "          13     0.9542    0.9670    0.9606     12252\n",
      "          14     0.9943    0.9951    0.9947     88553\n",
      "          15     0.9891    0.9849    0.9870     30891\n",
      "\n",
      "   micro avg     0.9825    0.9825    0.9825    370071\n",
      "   macro avg     0.9279    0.9028    0.9117    370071\n",
      "weighted avg     0.9825    0.9825    0.9825    370071\n",
      " samples avg     0.9825    0.9825    0.9825    370071\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#model evaluation (classic meters)\n",
    "\n",
    "pred = model.predict({'input_ids':xtest['input_ids'],'attention_mask':xtest['attention_mask']})\n",
    "\n",
    "y_pred = np.argmax(pred, axis = 1)\n",
    "y_pred = to_categorical(y_pred, num_classes=classes_num).astype('int32')\n",
    "\n",
    "print(classification_report(ytest, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b9fd3a-da11-415d-9f69-27e33f7dce91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75fa1f45-ca47-499a-be0c-80a552c0539a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "5975/5975 [==============================] - 1908s 319ms/step - loss: 0.4097 - balanced_accuracy: 0.8732 - val_loss: 0.1703 - val_balanced_accuracy: 0.9531\n",
      "Epoch 2/6\n",
      "5975/5975 [==============================] - 1897s 317ms/step - loss: 0.1516 - balanced_accuracy: 0.9577 - val_loss: 0.1320 - val_balanced_accuracy: 0.9643\n",
      "Epoch 3/6\n",
      "5975/5975 [==============================] - 1915s 320ms/step - loss: 0.1176 - balanced_accuracy: 0.9676 - val_loss: 0.1241 - val_balanced_accuracy: 0.9667\n",
      "Epoch 4/6\n",
      "5975/5975 [==============================] - 1932s 323ms/step - loss: 0.0980 - balanced_accuracy: 0.9734 - val_loss: 0.1227 - val_balanced_accuracy: 0.9678\n",
      "Epoch 5/6\n",
      "5975/5975 [==============================] - 1919s 321ms/step - loss: 0.0837 - balanced_accuracy: 0.9774 - val_loss: 0.1258 - val_balanced_accuracy: 0.9683\n",
      "Epoch 6/6\n",
      "5975/5975 [==============================] - 1906s 319ms/step - loss: 0.0722 - balanced_accuracy: 0.9807 - val_loss: 0.1243 - val_balanced_accuracy: 0.9687\n"
     ]
    }
   ],
   "source": [
    "#train the model (all meters)\n",
    "\n",
    "train_history = model.fit(\n",
    "    x ={'input_ids':xtrain['input_ids'],'attention_mask':xtrain['attention_mask']}, y = ytrain,\n",
    "    validation_data = ({'input_ids':xtest['input_ids'],'attention_mask':xtest['attention_mask']}, \n",
    "    ytest), epochs=6, batch_size=train_batch_size )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "150ba65a-e966-4b63-b3ec-e8f04bb33de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11950/11950 [==============================] - 646s 54ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9896    0.9906    0.9901     53009\n",
      "           1     0.7554    0.5926    0.6641      1480\n",
      "           2     0.8851    0.8927    0.8889       587\n",
      "           3     0.7811    0.8381    0.8086      1686\n",
      "           4     1.0000    0.1000    0.1818        10\n",
      "           5     0.8175    0.8063    0.8119      1833\n",
      "           6     0.9791    0.9868    0.9830     81382\n",
      "           7     0.0000    0.0000    0.0000        12\n",
      "           8     0.9764    0.9906    0.9834     34821\n",
      "           9     0.0000    0.0000    0.0000         1\n",
      "          10     0.9106    0.8650    0.8872      1755\n",
      "          11     0.6325    0.4086    0.4965       257\n",
      "          12     0.5846    0.5040    0.5413       377\n",
      "          13     0.7350    0.6041    0.6632      7682\n",
      "          14     0.3462    0.2500    0.2903        72\n",
      "          15     0.9188    0.9359    0.9273      3871\n",
      "          16     0.9477    0.9645    0.9560      5973\n",
      "          17     0.8579    0.8263    0.8418       190\n",
      "          18     0.8411    0.8703    0.8554      1326\n",
      "          19     0.9814    0.9735    0.9774     13905\n",
      "          20     0.9314    0.9231    0.9272     22267\n",
      "          21     0.9334    0.9519    0.9426     17971\n",
      "          22     1.0000    0.3333    0.5000         3\n",
      "          23     0.9438    0.9645    0.9541     12252\n",
      "          24     0.6992    0.4550    0.5513       189\n",
      "          25     0.9938    0.9946    0.9942     88553\n",
      "          26     0.9820    0.9863    0.9841     30891\n",
      "          27     0.2308    0.1875    0.2069        16\n",
      "\n",
      "   micro avg     0.9687    0.9687    0.9687    382371\n",
      "   macro avg     0.7734    0.6856    0.7075    382371\n",
      "weighted avg     0.9675    0.9687    0.9679    382371\n",
      " samples avg     0.9687    0.9687    0.9687    382371\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#model evaluation (all meters)\n",
    "\n",
    "pred = model.predict({'input_ids':xtest['input_ids'],'attention_mask':xtest['attention_mask']})\n",
    "\n",
    "y_pred = np.argmax(pred, axis = 1)\n",
    "y_pred = to_categorical(y_pred, num_classes=classes_num).astype('int32')\n",
    "\n",
    "print(classification_report(ytest, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4547a78-e197-4212-8211-700f28a017c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2b334a-63ed-4bf2-a31c-7b78073be5c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7339e2-eb16-4e53-b7e8-d61654aa18f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# classifier_path = 'finetuned/classic_meters_classifierTF_araBERT.h5'\n",
    "classifier_path = 'finetuned/all_meters_classifierTF_araBERT.h5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5185399-536e-4687-aa72-093cdcc76c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving finetuned model locally\n",
    "\n",
    "model.save_weights(classifier_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b486d78-e842-4dc6-a06e-11a1414452ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the finetuned model locally\n",
    "# it's necessary to create new model similar to the saved one, then load it\n",
    "\n",
    "from transformers import TFBertModel\n",
    "\n",
    "def create_model():\n",
    "    input_ids = Input(shape=(max_sequence_length,), dtype=tf.int32, name=\"input_ids\")\n",
    "    input_mask = Input(shape=(max_sequence_length,), dtype=tf.int32, name=\"attention_mask\")\n",
    "    bert = TFBertModel.from_pretrained('aubmindlab/bert-base-arabert', from_pt=True)\n",
    "    \n",
    "    output = bert([input_ids, input_mask])[1] #pooled_output\n",
    "    output = tf.keras.layers.Dense(classes_num, activation='softmax', name='output')(output)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[input_ids, input_mask], outputs=output)\n",
    "    \n",
    "    optimizer = Adam(learning_rate=5e-05)\n",
    "     \n",
    "    loss =CategoricalCrossentropy(from_logits = True)\n",
    "    metric = CategoricalAccuracy('balanced_accuracy'),\n",
    "    \n",
    "    model.compile(optimizer = optimizer, loss = loss, metrics = metric)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.load_weights(classifier_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d068da-ad12-4531-b7fa-7a0d3029c08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model evaluation\n",
    "\n",
    "pred = model.predict({'input_ids':xtest['input_ids'],'attention_mask':xtest['attention_mask']})\n",
    "\n",
    "y_pred = np.argmax(pred, axis = 1)\n",
    "y_pred = to_categorical(y_pred, num_classes=classes_num).astype('int32')\n",
    "\n",
    "print(classification_report(ytest, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9c865f-e42d-4977-a783-58ebdebb7318",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# further finetuning the model\n",
    "\n",
    "train_history = model.fit(\n",
    "    x ={'input_ids':xtrain['input_ids'],'attention_mask':xtrain['attention_mask']}, y = ytrain,\n",
    "    validation_data = ({'input_ids':xtest['input_ids'],'attention_mask':xtest['attention_mask']}, \n",
    "    ytest), epochs=3, batch_size=train_batch_size )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec7cc85-d431-483b-9ceb-f0eb0c12f1fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a11ab3-0772-4547-aac4-14b9753f9864",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
