{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb12e46f-23c8-4181-9e24-a0276dc12dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy==1.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4da4c2c-e801-4fa6-8676-c04e551ee88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.12\n",
    "!pip install pyarabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99b7f2d-e4a6-4dc1-9a9f-eb0d99dc95b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbd9b83-c345-489a-ba9d-c27dc601972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets\n",
    "!pip install datasets\n",
    "!pip install transformers[torch]\n",
    "!pip install nvidia-ml-py3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c51d229-d74c-4ce9-9910-786a65f81dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccb60e6e-ebd4-4a2d-96f8-8b5f2c65d151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-28 08:18:40.084985: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-28 08:18:40.112272: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-28 08:18:40.532370: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2090907"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2090907"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# cell-1  \n",
    "#load and clean the data (removing diacritics and unwanted text)\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import pyarabic.araby as araby\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "\n",
    "df = pd.read_csv('poemsDataset.csv')\n",
    "df.fillna('', inplace=True)\n",
    "display(len(df))\n",
    "\n",
    "\n",
    "def remove_diacritics(a):    \n",
    "    return araby.strip_diacritics(a)\n",
    "\n",
    "df['first_hemistich'] = df['first_hemistich'].apply(remove_diacritics)\n",
    "df['second_hemistich'] = df['second_hemistich'].apply(remove_diacritics)\n",
    "\n",
    "def normalizeBeforeTraining(df):\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('النابغـة: ', '')\n",
    "    df['second_hemistich'] = df['second_hemistich'].str.replace('الـربيع: ', '')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('عبيــد: ', '')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('امـرؤ القيسـ: ', '')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('امرؤ القيس: ', '')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('(جلال الــــدين الــــرومي):', '')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('(لـوك الفيلسـوف الإنكليزي):', '')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('(كانت الفيلسوف الألماني ):', '')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('(بركســــــــــــــــون):', '')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('(الحـــــــــــــــور):', '')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('(الشــــــــــــــاعر):', '')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('(الإنســـــــــــــــان):', '')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('العلم):', '', regex=False)\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('(العشــــــــــــــــق):', '', regex=False)\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('(الزهــــــــــــــــــرة):', '', regex=False)\n",
    "    df['second_hemistich'] = df['second_hemistich'].str.replace('التوأم اليشكري: ', '', regex=False)  \n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('آ', 'أ')\n",
    "    df['second_hemistich'] = df['second_hemistich'].str.replace('آ', 'أ')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('[/\":?،؟]', '')\n",
    "    df['second_hemistich'] = df['second_hemistich'].str.replace('[/\":?،؟]', '')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('  ', ' ')\n",
    "    df['second_hemistich'] = df['second_hemistich'].str.replace('  ', ' ')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('  ', ' ')\n",
    "    df['second_hemistich'] = df['second_hemistich'].str.replace('  ', ' ')\n",
    "\n",
    "\n",
    "normalizeBeforeTraining(df)\n",
    "df.drop(df[(df['first_hemistich'] == '') & (df['second_hemistich'] == '')].index, inplace=True)\n",
    "\n",
    "#if first_hemistich == '', then copy the text from second_hemistich. then delete the text in the second_hemistich\n",
    "df['first_hemistich'] = df.apply(lambda x: x['second_hemistich'] if x['first_hemistich'] == '' else x['first_hemistich'], axis=1)\n",
    "df['second_hemistich'] = df.apply(lambda x: '' if x['first_hemistich'] == x['second_hemistich'] else x['second_hemistich'], axis=1)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "display(len(df))\n",
    "# display(df[:10])\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "610bdc76-f731-483e-9663-4e9a54c7073e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21230\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "315877"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "252701"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "63176"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cell-2 \n",
    "# preparing data for finetuning\n",
    "\n",
    "\n",
    "df['second_hemistich'].replace('', 'E', inplace=True)\n",
    "dfc = df[['first_hemistich', 'second_hemistich', 'type_en', 'link']].copy()\n",
    "dfc['text'] = dfc['first_hemistich'] + ' S ' + dfc['second_hemistich']\n",
    "\n",
    "\n",
    "dfc['sent'] = ''\n",
    "\n",
    "dfc.loc[dfc['type_en'] == 'romantic poems', 'sent'] = \"love\"  \n",
    "dfc.loc[dfc['type_en'] == 'parting poems', 'sent'] = \"love\"\n",
    "dfc.loc[dfc['type_en'] == 'longing poems', 'sent'] = \"love\"\n",
    "dfc.loc[dfc['type_en'] == 'spinning poems', 'sent'] = \"love\"\n",
    "dfc.loc[dfc['type_en'] == 'elegy poems', 'sent'] = \"sad\"   \n",
    "dfc.loc[dfc['type_en'] == 'slander poems', 'sent'] = \"angry\"   \n",
    "dfc.loc[dfc['type_en'] == 'religious poems', 'sent'] = \"religious\"\n",
    "dfc.loc[dfc['type_en'] == 'invocation poems', 'sent'] = \"religious\"\n",
    "dfc.loc[dfc['type_en'] == 'mercy poems', 'sent'] = \"religious\"\n",
    "\n",
    "\n",
    "dfc = dfc[dfc['sent'] != ''] \n",
    "\n",
    "\n",
    "\n",
    "dfc.reset_index(drop=True, inplace=True)\n",
    "\n",
    "dfc['sent'] = dfc['sent'].astype('category')\n",
    "\n",
    "\n",
    "dfc['label'] = dfc['sent'].cat.codes #assign cat_value for each sentiment type\n",
    "dftrain, dftest = train_test_split(dfc, test_size=0.20, random_state=42, stratify=dfc['label'])\n",
    "\n",
    "ytrain = dftrain['label'].values.tolist()\n",
    "ytest = dftest['label'].values.tolist()\n",
    "\n",
    "z = set(dfc['link'].values.tolist())\n",
    "print(len(z)) #number of poems\n",
    "\n",
    "max_sequence_length = 32\n",
    "train_batch_size = 256\n",
    "classes_num = len(dfc['sent'].unique())\n",
    "\n",
    "display(classes_num)\n",
    "display(len(dfc))\n",
    "display(len(dftrain))\n",
    "display(len(dftest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdae2e7c-370e-407c-a87a-2e61f782f399",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at aubmindlab/bert-base-arabert were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabert and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#cell-3\n",
    "#loading the tokenizer and the model\n",
    "\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('aubmindlab/bert-base-arabert')\n",
    "model = BertForSequenceClassification.from_pretrained('aubmindlab/bert-base-arabert',\n",
    "                                                      num_labels=classes_num).to('cuda')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd261fd0-0b55-46b6-8f5a-67f213baeb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cell-4\n",
    "#tokenizing the data\n",
    "\n",
    "xtrain = tokenizer(\n",
    "    text=dftrain['text'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length = max_sequence_length,\n",
    "    truncation=True,\n",
    "    padding='max_length', \n",
    "    return_tensors='pt',\n",
    "    return_token_type_ids = False,\n",
    "    return_attention_mask = True,\n",
    "    verbose = True)\n",
    "\n",
    "\n",
    "xtest = tokenizer(\n",
    "    text=dftest['text'].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length = max_sequence_length,\n",
    "    truncation=True,\n",
    "    padding='max_length', \n",
    "    return_tensors='pt',\n",
    "    return_token_type_ids = False,\n",
    "    return_attention_mask = True,\n",
    "    verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d176250-d479-4fb9-be02-9202926c40a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#group text and labels and create train_ds and test_ds\n",
    "\n",
    "import torch\n",
    "\n",
    "class NewGroupDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item['label'] = torch.tensor([self.labels[idx]])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_ds = NewGroupDataset(xtrain, ytrain)\n",
    "test_ds = NewGroupDataset(xtest, ytest)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return { 'accuracy': acc }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f46687a1-5cab-49fc-ad77-d20c5667459e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abd8fb74-d45d-46c7-bb99-22968f47a230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3952' max='3952' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3952/3952 04:31, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.767800</td>\n",
       "      <td>0.704246</td>\n",
       "      <td>0.751741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.668100</td>\n",
       "      <td>0.694851</td>\n",
       "      <td>0.753593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.588100</td>\n",
       "      <td>0.714517</td>\n",
       "      <td>0.750791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.511200</td>\n",
       "      <td>0.756444</td>\n",
       "      <td>0.743352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3952, training_loss=0.6274845841442526, metrics={'train_runtime': 272.152, 'train_samples_per_second': 3714.115, 'train_steps_per_second': 14.521, 'total_flos': 1.6622405186509824e+16, 'train_loss': 0.6274845841442526, 'epoch': 4.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "epochs = 4\n",
    "save_steps = 10000 #save checkpoint every 10000 steps\n",
    "batch_size = 256\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = 'araBERT_base_poemsType/',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs = epochs,\n",
    "    per_device_train_batch_size = batch_size,\n",
    "    per_device_eval_batch_size = batch_size,\n",
    "    save_steps = save_steps,\n",
    "    save_total_limit = 5, #only save the last 5 checkpoints\n",
    "    fp16=True,\n",
    "    learning_rate = 5e-5,  # 5e-5 is the default\n",
    "    logging_steps = 500, #50_000\n",
    "    evaluation_strategy = 'epoch',\n",
    "    # evaluate_during_training = True,\n",
    "    eval_steps = 300\n",
    "    \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    # data_collator=data_collator,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "\n",
    "# trainer.train(resume_from_checkpoint=True)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb133ac-beda-4437-b1db-3b5a6beab4ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c43aa7fb-c909-41ac-a427-e2a499c8df9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('araBERT_base_poemsType/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c30df8-1cf0-4ad2-bbfe-8b16c80b0c53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a00f8eb3-0d6c-4a14-b189-2dffb96207b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = trainer.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1feeae5-a917-4f8d-9a2e-18e6b6fd1339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068c3a1e-14b2-422b-8c24-8350a841d90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(p[0], axis = 1)\n",
    "\n",
    "print(classification_report(ytest, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c24056-79a2-4513-aca9-93fee62e5051",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "matrix = confusion_matrix(ytest, y_pred)\n",
    "cmn = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]\n",
    "cmn = cmn*100\n",
    "fig, ax = plt.subplots(figsize=(17,17))\n",
    "sn.set(font_scale=1)\n",
    "sn.heatmap(cmn, annot=True, fmt='.1f', xticklabels=range(classes_num), yticklabels=range(classes_num), cmap=\"Blues\",\n",
    "          annot_kws={\"size\": 12})\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.savefig('poem_types_cm.png')\n",
    "plt.show(block=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2939732-29e5-44cc-9136-fe8cdac3afe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc['m'] = dfc['sent'].astype(str) + ' ' + dfc['label'].astype(str)\n",
    "df2 = dfc.groupby(['m'])['first_hemistich'].count().sort_values()\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8ecbf0-c481-4f21-b7c8-c4d896640e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d485be6b-0c9b-4cf4-93ba-9a339b0ca6f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
