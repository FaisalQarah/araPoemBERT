{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb12e46f-23c8-4181-9e24-a0276dc12dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy==1.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4da4c2c-e801-4fa6-8676-c04e551ee88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.12\n",
    "!pip install pyarabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99b7f2d-e4a6-4dc1-9a9f-eb0d99dc95b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a42c50-df29-4c17-84f2-69da839d31e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets\n",
    "!pip install datasets\n",
    "!pip install transformers[torch]\n",
    "!pip install nvidia-ml-py3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a49286b-4163-4b95-9c68-fdfe6aaa3dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122dc8fb-4fb8-4065-8231-8cf429b81a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import pyarabic.araby as araby\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import TruncatedNormal\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccb60e6e-ebd4-4a2d-96f8-8b5f2c65d151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2090907"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2090907"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# cell-1\n",
    "#load and clean the data (removing diacritics and unwanted text)\n",
    "\n",
    "df = pd.read_csv('poemsDataset.csv')\n",
    "df.fillna('', inplace=True)\n",
    "display(len(df))\n",
    "\n",
    "\n",
    "def remove_diacritics(a):    \n",
    "    return araby.strip_diacritics(a)\n",
    "\n",
    "df['first_hemistich'] = df['first_hemistich'].apply(remove_diacritics)\n",
    "df['second_hemistich'] = df['second_hemistich'].apply(remove_diacritics)\n",
    "\n",
    "def normalizeBeforeTraining(df):\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('النابغـة: ', '')\n",
    "    df['second_hemistich'] = df['second_hemistich'].str.replace('الـربيع: ', '')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('عبيــد: ', '')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('امـرؤ القيسـ: ', '')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('امرؤ القيس: ', '')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('(جلال الــــدين الــــرومي):', '')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('(لـوك الفيلسـوف الإنكليزي):', '')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('(كانت الفيلسوف الألماني ):', '')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('(بركســــــــــــــــون):', '')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('(الحـــــــــــــــور):', '')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('(الشــــــــــــــاعر):', '')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('(الإنســـــــــــــــان):', '')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('العلم):', '', regex=False)\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('(العشــــــــــــــــق):', '', regex=False)\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('(الزهــــــــــــــــــرة):', '', regex=False)\n",
    "    df['second_hemistich'] = df['second_hemistich'].str.replace('التوأم اليشكري: ', '', regex=False)  \n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('آ', 'أ')\n",
    "    df['second_hemistich'] = df['second_hemistich'].str.replace('آ', 'أ')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('[/\":?،؟]', '')\n",
    "    df['second_hemistich'] = df['second_hemistich'].str.replace('[/\":?،؟]', '')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('  ', ' ')\n",
    "    df['second_hemistich'] = df['second_hemistich'].str.replace('  ', ' ')\n",
    "    df['first_hemistich'] = df['first_hemistich'].str.replace('  ', ' ')\n",
    "    df['second_hemistich'] = df['second_hemistich'].str.replace('  ', ' ')\n",
    "\n",
    "\n",
    "normalizeBeforeTraining(df)\n",
    "df.drop(df[(df['first_hemistich'] == '') & (df['second_hemistich'] == '')].index, inplace=True)\n",
    "\n",
    "#if first_hemistich == '', then copy the text from second_hemistich. then delete the text in the second_hemistich\n",
    "df['first_hemistich'] = df.apply(lambda x: x['second_hemistich'] if x['first_hemistich'] == '' else x['first_hemistich'], axis=1)\n",
    "df['second_hemistich'] = df.apply(lambda x: '' if x['first_hemistich'] == x['second_hemistich'] else x['second_hemistich'], axis=1)\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "display(len(df))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e68754b-15f2-4e08-b38a-ea0ccd974d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell-2\n",
    "# preparing data for pretraining (all poems even those without labeled meter are used)\n",
    "\n",
    "df['second_hemistich'].replace('', 'E', inplace=True)\n",
    "dfc = df[['first_hemistich', 'second_hemistich', 'meter']].copy()\n",
    "dfc['text'] = dfc['first_hemistich'] + ' S ' + dfc['second_hemistich']\n",
    "\n",
    "dfc.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0f2152-8305-48ab-b3d2-d998c401d840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5835f0b-4bb6-413c-882a-79ac20f5433a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d099a0-b687-48a4-bbb3-6e0e9535340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#don't run unless there isn't a pretrained tokenizer\n",
    "#train tokenizer (using iterator function), then save tokenizer locally\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "#loading bert tokenizer to work as a base for the new tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def batch_iterator(batch_size=10000):\n",
    "    for i in tqdm(range(0, len(dfc), batch_size)):\n",
    "        yield dfc[i: i +batch_size]['text']\n",
    "bert_tokenizer = tokenizer.train_new_from_iterator(text_iterator=batch_iterator(), vocab_size=50000)\n",
    "bert_tokenizer.save_pretrained('araPoemBERT_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fbc2cf-592b-4274-9747-0cef3c5e27bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "376680b8-f386-4505-9e5e-fe8238fa293b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2090851"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cell-3\n",
    "#tokenizing the whole text \n",
    "\n",
    "import tokenizers\n",
    "from transformers import Trainer, TrainingArguments, LineByLineTextDataset, BertModel\n",
    "from transformers import BertConfig, BertForMaskedLM, DataCollatorForLanguageModeling\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('faisalq/bert-medium-arapoembert')\n",
    "max_seq_length = 32\n",
    "\n",
    "dataset = LineByLineTextDataset(tokenizer = tokenizer, file_path = 'poem_text.txt', \n",
    "                               block_size = max_seq_length)\n",
    "display(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24d8d939-13cc-499d-98bb-e663a67d81ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_ids': tensor([    2,   909,  3603,    53,  1472,  8993,     9,   126,  8979,  8989,\n",
       "          11435,   133,     3])},\n",
       " {'input_ids': tensor([    2,   126,  1094, 15305,  9899,   133,     9,  1599,   136,  1814,\n",
       "           2611,  5667,     3])},\n",
       " {'input_ids': tensor([    2, 17860, 13222, 13908,  1684,     9, 11696, 11360,   126,  3682,\n",
       "             85, 14474,     3])},\n",
       " {'input_ids': tensor([    2,   124,   609,  6877,  3410,   340, 31589,     9,     8,     3])},\n",
       " {'input_ids': tensor([    2,    92, 11630,  8859,    84, 26039,     9, 31405,   167,   121,\n",
       "           4450, 42093,    51,     3])}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ec45482-b6f0-4281-9f55-829fa6d1eb09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95772752"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cell-4\n",
    "# model config\n",
    "config = BertConfig( vocab_size = 50000, \n",
    "                    hidden_size = 768, \n",
    "                    num_hidden_layers = 8,\n",
    "                    num_attention_heads = 12,\n",
    "                    max_position_embeddings = 32)\n",
    "\n",
    "model = BertForMaskedLM(config)\n",
    "display(model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3adb1308-d307-40a5-af17-6fbd936a5701",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10561224' max='10561224' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10561224/10561224 01:06, Epoch 1293/1293]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10560300</td>\n",
       "      <td>2.124900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10560600</td>\n",
       "      <td>2.148500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10560900</td>\n",
       "      <td>2.136600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10561200</td>\n",
       "      <td>2.149300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cell-5\n",
    "#pretraining the model\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True,\n",
    "                                               mlm_probability=0.15)\n",
    "epochs = 1293\n",
    "save_steps = 10000 #save checkpoint every 10000 steps\n",
    "batch_size = 256\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = 'araPoemBERT_8L12H/',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs = epochs,\n",
    "    per_device_train_batch_size = batch_size,\n",
    "    save_steps = save_steps,\n",
    "    save_total_limit = 5, #only save the last 5 checkpoints\n",
    "    fp16=True,\n",
    "    learning_rate = 5e-5,  # 5e-5 is the default\n",
    "    logging_steps = 300 #50_000\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "from pynvml import *\n",
    "\n",
    "\n",
    "trainer.train(resume_from_checkpoint=True)\n",
    "# trainer.train()\n",
    "trainer.save_model('araPoemBERT_8L12H/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55c6051e-0616-46de-b380-fb00e8630c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('araPoemBERT_8L12H/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4c12f2-0702-4b4b-a622-5878ae280109",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
